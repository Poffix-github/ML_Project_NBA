{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "games = pd.read_csv('../data/') # TODO: add dataset name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = ['elo_difference', 'WIN_PCT_10_home','WIN_PCT_10_away', 'FG_PCT_avg_10_HOME', \n",
    "            'FG_PCT_avg_10_AWAY', 'FT_PCT_avg_10_HOME', \n",
    "            'FT_PCT_avg_10_AWAY', 'FG3_PCT_avg_10_HOME', 'FG3_PCT_avg_10_AWAY', 'REB_avg_10_HOME', 'REB_avg_10_AWAY']\n",
    "\n",
    "# Separate the features and the target\n",
    "X = games[features]\n",
    "y = games['HOME_TEAM_WINS']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the features\n",
    "scaler.fit(X)\n",
    "# Transform the features using the scaler\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rdf = RandomForestClassifier(n_estimators=100)  \n",
    "# n_estimator=1000 better accuracy but too much time\n",
    "rdf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = rdf.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "\n",
    "# Print the feature names and importances\n",
    "for feature, importance in zip(X.columns, sorted_importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = gnb.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "\n",
    "# Print the feature names and importances\n",
    "for feature, importance in zip(X.columns, sorted_importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrg = LogisticRegression(penalty=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`penalty='l2'` is very close but consistently worse. By GridSearchCV.\n",
    "\n",
    "|params|None|l2|\n",
    "|------|----|--|\n",
    "|mean_test_AUC|0.69534876|0.69481222|\n",
    "|mean_train_AUC|0.69622475|0.69620209|\n",
    "|mean_test_Accuracy|0.66015328|0.65864726|\n",
    "|mean_train_Accuracy|0.65961871|0.65985651|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = lrg.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "\n",
    "# Print the feature names and importances\n",
    "for feature, importance in zip(X.columns, sorted_importances):\n",
    "    print(f\"{feature}: {importance:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dmc = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "\n",
    "dmc.fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment\n",
    "\n",
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def cross_validation(model, X, y, cvn=10):\n",
    "    \"\"\"\n",
    "    Proxy for cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_val_score(model, X, y, cv = StratifiedKFold(n_splits = cvn))\n",
    "    return scores.mean(), scores.std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix_data(model, X_test, y_test, name):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix of the given model.\n",
    "    Export a graph for the matrix. \n",
    "    Return confusion matrix array containing tn, fp, fn and tp.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predictions, labels=model.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                display_labels=model.classes_)\n",
    "    # export graph\n",
    "    disp.plot()\n",
    "    plt.savefig(fname='../iterations_evaluation/'+name+'_confusion_matrix')\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve, RocCurveDisplay\n",
    "\n",
    "def roc_data(y, pred, name):\n",
    "    \"\"\"\n",
    "    Compute ROC information for the given model. \n",
    "    Export a graph for the curve. Return AUC value.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y, pred)\n",
    "    print(fpr, tpr)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                   estimator_name='wins')\n",
    "    # export graph\n",
    "    display.plot()\n",
    "    plt.savefig(fname='../iterations_evaluation/'+name+'_roc')\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr_fnr_data(cm):\n",
    "    \"\"\"\n",
    "    Compute False Positive Rate and False Negative Rate from confusion matrix.\n",
    "    \"\"\"\n",
    "    # the confusion matrix has True Negative, False Positive, False Negative \n",
    "    # and True Positive info.\n",
    "    tn, fp, fn, tp = cm.ravel() # .ravel() flattens the array to a 1-Dimension array    # TODO: needs numpy?\n",
    "\n",
    "    # To get the total number of wins and losses we count the ones and zeroes, \n",
    "    # respectively, from the Target variable.\n",
    "    wins, losses = y.value_counts()\n",
    "\n",
    "    fpr = fp/losses     # FPR = FP/N\n",
    "    fnr = fn/wins       # FNR = FN/P\n",
    "\n",
    "    return fpr, fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "\n",
    "def indexes(model, y, X_test, y_test, name):\n",
    "    \"\"\"\n",
    "    Compute accuracy, FPR, FNR, confusion matrix and ROC curve.\n",
    "    Exports two images, one for the confusion matrix anthe other for ROC; \n",
    "    all other indexes are added to a json file.\n",
    "    \"\"\"\n",
    "    # Indexes computation\n",
    "    accuracy = cross_val_score(model, X, y, cv = StratifiedKFold(n_splits = 10))    # accuracy\n",
    "    cm = confusion_matrix_data(model, X_test, y_test, name)             # confusion matrix\n",
    "    fpr, fnr = fpr_fnr_data(cm)                                         # False Positive Rate, False Negative Rate\n",
    "    area_under_curve = roc_data(y, model.predict_proba(X)[:, 1], name)  # Receiver Operating Characteristic curve and Area Under the Curve\n",
    "    \n",
    "    # Indexes export handling\n",
    "    json_filename = '../iterations_evaluation/iterations.json'\n",
    "\n",
    "    if path.isfile(json_filename) is True:  # file exists\n",
    "        # get json file\n",
    "        with open(file=json_filename,mode='r') as f: # read mode\n",
    "            all_indexes = json.load(f)  # json file is translated to a python dictionary\n",
    "    else:   # file does not exists\n",
    "        with open(file=json_filename,mode='x') as f: # create mode\n",
    "            all_indexes = dict()    # create an empty dictionary\n",
    "    # add new indexes to dictionary\n",
    "    all_indexes.update({\n",
    "        name: {\n",
    "            'accuracy': [accuracy.mean(), accuracy.std()],\n",
    "            'confusion_matrix': [x.item() for x in cm.ravel()], # cm is numpy.ndarray and its elements are numpy.int64; both types aren't \"JSON Serialiazable\" so they have to be translated to native Python types\n",
    "            'FPR': fpr,\n",
    "            'FNR': fnr,\n",
    "            'AUC': area_under_curve\n",
    "        }\n",
    "    })\n",
    "    # rewrite file with new indexes\n",
    "    with open(file=json_filename,mode='w') as f:\n",
    "        json.dump(all_indexes, f, indent=4, separators=(',',': '))\n",
    "    \n",
    "    return accuracy, cm, fpr, fnr, area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = indexes(dmy, y, X_test, y_test, 'dummy_final')\n",
    "randomFor = indexes(rdf, y, X_test, y_test, 'RndFor_final')\n",
    "NaiveBayes = indexes(gnb, y, X_test, y_test, 'NaiveBayes_final')\n",
    "LogisticRegres = indexes(lrg, y, X_test, y_test, 'LogiRegre_final')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0c65ca2b5f57550b7f4b24ac9925e993a236f50d0e8e5a8a4ba8b6733c01578"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
